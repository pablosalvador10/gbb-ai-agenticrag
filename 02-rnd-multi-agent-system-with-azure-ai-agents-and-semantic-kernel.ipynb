{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use Case Overview**\n",
    "\n",
    "Imagine you’re part of an R&D team that needs to merge structured data (e.g., experimental results, market trends) from Microsoft Fabric with unstructured documents (e.g., research reports, engineering notes) in SharePoint, then validate these findings against external references (e.g., Bing) and a high-quality “ground truth” internal knowledge stores (e.g., Azure AI Search).\n",
    "\n",
    "This was a classic Retrieval-Augmented Generation (RAG) scenario—multiple data sources must be queried in real time and cross-checked for consistency. However, by leveraging Azure AI agent services (an agetic enterprise-ready microservices approach) alongside frameworks like Semantic Kernel, we can evolve beyond basic RAG into a mostly autonomous, agentic system. In this design, Agentic RAG and the Reflection Pattern enable each agent to iteratively refine its output until it’s confident in delivering a high-quality, validated answer—paving the way for intelligent automation that continually learns and improves.\n",
    "\n",
    "To summarize, you’re not only bringing data to the AI but also bringing AI to the data, thus maximizing the value of your knowledge stores. By leveraging state-of-the-art retrieval solutions like Azure AI Search, while also tapping sources such as SharePoint (unstructured data) and Fabric (structured data), you can harness your most valuable asset—data—to achieve new levels of insight and automation. \n",
    "\n",
    "**In this demo, we have two Azure AI Agents (extending beyond a single-agent architecture):**\n",
    "\n",
    "+ DataRetrievalAgent: Has access to Microsoft Fabric (for structured data) and SharePoint (for unstructured documents). Its job is to gather relevant internal data: for example, “failure rates of Material X in high-temperature tests,” or “engineering notes on prior tests.”\n",
    "\n",
    "- ValidationInsightsAgent Has access to Bing / Azure Cognitive Search for external references and can run a “reflection” or “validation” step. Its job is to cross-check what was returned by the first agent and highlight missing or conflicting information. ValidationInsightsAgent has access to highly curated knowledge sources (e.g., Azure AI Search) for validating the accuracy or truthfulness of the information it receives from DataRetrievalAgent.\n",
    "\n",
    "\n",
    "**Moving from a single-agent setup to a multi-agent system is now simpler than ever with Semantic Kernel. The general flow looks like this:**\n",
    "\n",
    "1. The user asks a question (e.g., “Retrieve historical failure rates for Material X in extreme temperatures and cross-check if new standards or conflicting data exist.”).\n",
    "2. The DataRetrievalAgent fetches structured data from Fabric (e.g., lab test results, analytics) and unstructured docs from SharePoint (e.g., research memos, engineering notes).\n",
    "3. The ValidationInsightsAgent then queries Bing/Azure Search to verify or supplement the results. Employ a reflection pattern, where it iterates over the combined results, looking for gaps or inconsistencies. If needed, it loops back to the DataRetrievalAgent for clarifications or additional data.\n",
    "\n",
    "Finally, the user receives a validated, summarized answer that merges internal data with external cross-checks. Thanks to the agents’ back-and-forth reflection.\n",
    "\n",
    "### **Why This Matters**\n",
    "\n",
    "+ **Reduced Manual Research**: Instead of manually sifting through multiple data silos and external search engines, the AI Agents automate data gathering and vetting.\n",
    "+ **Higher Confidence**: Validation ensures data accuracy and highlights missing pieces, improving R&D decision-making.\n",
    "+ **Enterprise-Grade Security**: Each agent can enforce On-Behalf-Of (OBO) authentication to protect sensitive data (e.g., only pulling data the user is authorized to see).\n",
    "In the Jupyter Notebook\n",
    "\n",
    "When you run the code in this Jupyter notebook:\n",
    "\n",
    "(TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime as pydatetime\n",
    "from typing import Any, List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "\n",
    "# Azure AI Projects\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from semantic_kernel.agents.azure_ai import AzureAIAgent, AzureAIAgentSettings\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# configure logging\n",
    "from utils.ml_logging import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "# helper functions\n",
    "from utils.utilityfucntions import print_agent_summary\n",
    "\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    target_directory = os.getenv(\"TARGET_DIRECTORY\", os.getcwd())  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Client and Load Azure AI Foundry**\n",
    "\n",
    "Here, we initialize the Azure AI client using DefaultAzureCredential. This allows us to authenticate and connect to the Azure AI service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AzureAIAgent.create_client(credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Creating Azure AI Agents: DataRetrievalAgent**\n",
    "\n",
    "The DataRetrievalAgent is responsible for internal data retrieval, combining structured data from Microsoft Fabric with unstructured documents from SharePoint. This agent ensures that research teams can efficiently access critical R&D insights, such as historical failure rates, experimental results, and engineering notes—all while maintaining secure and authorized access controls.\n",
    "\n",
    "Agent Capabilities\n",
    "+ ✅ Structured Data Retrieval → Queries Microsoft Fabric for experiment logs, test results, and structured analytics.\n",
    "+ ✅ Unstructured Document Search → Fetches relevant reports, blueprints, and research notes from SharePoint.\n",
    "+ ✅ OBO Authentication → Uses On-Behalf-Of (OBO) authentication to ensure users can only access data they are permitted to view.\n",
    "\n",
    "For a detailed breakdown of how to create a single Azure AI Agent and configure its data (tools) connections, please refer to:\n",
    "📌 [01-single-agents-with-azure-ai-agents.ipynb](01-single-agents-with-azure-ai-agents.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ServiceRequestError\n",
    "from azure.ai.projects.aio import AIProjectClient\n",
    "\n",
    "async def get_connection_id(client: AIProjectClient, env_var: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Retrieves the connection object using a connection name stored in an environment variable.\n",
    "\n",
    "    Args:\n",
    "        client: The Azure AI Project client.\n",
    "        env_var (str): The environment variable holding the connection name.\n",
    "\n",
    "    Returns:\n",
    "        Connection object if found, otherwise raises an error.\n",
    "    \"\"\"\n",
    "    connection_name = os.getenv(env_var)\n",
    "    if not connection_name:\n",
    "        logger.error(f\"Missing environment variable: '{env_var}'\")\n",
    "        raise ValueError(f\"Environment variable '{env_var}' is required.\")\n",
    "\n",
    "    try:\n",
    "        connection = await client.connections.get(connection_name=connection_name)\n",
    "        logger.info(f\"Retrieved Connection ID for {env_var}: {connection.id}\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to retrieve connection for {env_var}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 00:27:36,984 - micro - MainProcess - ERROR    Failed to create ToolSet: name 'get_connection_id' is not defined (4254263396.py:<module>:23)\n",
      "ERROR:micro:Failed to create ToolSet: name 'get_connection_id' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_connection_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m toolset = ToolSet()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Retrieve and add SharePoint Tool\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     sharepoint_connection = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mget_connection_id\u001b[49m(project_client, \u001b[33m\"\u001b[39m\u001b[33mTOOL_CONNECTION_NAME_SHAREPOINT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m     toolset.add(SharepointTool(connection_id=sharepoint_connection.id))\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# # Retrieve and add Fabric Tool\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# fabric_connection = await get_connection_id(project_client, \"TOOL_CONNECTION_NAME_FABRIC\")\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# toolset.add(FabricTool(connection_id=fabric_connection.id))\u001b[39;00m\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# logger.info(\"Successfully created ToolSet with SharePoint and Fabric tools.\")\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'get_connection_id' is not defined"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    SharepointTool,\n",
    "    FabricTool,\n",
    "    ToolSet,\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Agent settings\n",
    "dataretrievalagent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "toolset = ToolSet()\n",
    "\n",
    "try:\n",
    "    # Retrieve and add SharePoint Tool\n",
    "    sharepoint_connection = await get_connection_id(project_client, \"TOOL_CONNECTION_NAME_SHAREPOINT\")\n",
    "    toolset.add(SharepointTool(connection_id=sharepoint_connection.id))\n",
    "\n",
    "    # # Retrieve and add Fabric Tool\n",
    "    # fabric_connection = await get_connection_id(project_client, \"TOOL_CONNECTION_NAME_FABRIC\")\n",
    "    # toolset.add(FabricTool(connection_id=fabric_connection.id))\n",
    "\n",
    "    # logger.info(\"Successfully created ToolSet with SharePoint and Fabric tools.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create ToolSet: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataRetrievalAgent Run ID: asst_Wo0GJ9MpmvkfRPNwllC7bYFS\n"
     ]
    }
   ],
   "source": [
    "dataretrievalagent_settings_definition = await project_client.agents.create_agent(\n",
    "    model=dataretrievalagent_settings.model_deployment_name,\n",
    "    name=\"DataRetrievalAgent\",\n",
    "    description=(\n",
    "        \"An AI agent designed to retrieve and integrate structured data from Microsoft Fabric \"\n",
    "        \"and unstructured documents from SharePoint to provide comprehensive R&D insights.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"You are an AI assistant specialized in retrieving data from internal enterprise sources. \"\n",
    "        \"Utilize the provided tools to access and integrate information from Microsoft Fabric and SharePoint. \"\n",
    "        \"Ensure that all responses are based on the most relevant and recent data available.\"\n",
    "    ),\n",
    "    toolset=toolset,\n",
    "    headers={\"x-ms-enable-preview\": \"true\"},\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    metadata={\n",
    "        \"use_case\": \"Internal Data Retrieval for R&D\",\n",
    "        \"data_source\": \"Microsoft Fabric and SharePoint\",\n",
    "        \"response_validation\": \"Ensure data accuracy and relevance\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the agent's run ID (agent ID)\n",
    "print(f\"DataRetrievalAgent Run ID: {dataretrievalagent_settings_definition.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 **User:** What are the latest trends in R&D?\n",
      "\n",
      "🤖 **Agent:** The latest trends in research and development (R&D) in 2023 highlight significant advancements across various fields. Some of the notable trends include:\n",
      "\n",
      "1. **Diabetes Technology**:\n",
      "   - The approval of the Omnipod 5 automated insulin delivery (AID) system, which is a hybrid closed-loop insulin delivery system that uses a tubeless pod insulin pump connected to a continuous glucose monitoring (CGM) sensor via Bluetooth. This system helps modulate insulin delivery based on real-time glucose readings【9:1†source】.\n",
      "   - The clearance of the MiniMed 780G advanced hybrid closed-loop system with Guardian 4 sensor, focusing on improving glycemic outcomes for individuals with diabetes【7:1†source】.\n",
      "   - Development and approval of the Omnipod GO, a basal-only insulin pod designed to simplify insulin delivery for people with type 2 diabetes【7:1†source】.\n",
      "   - The increase in the use of diabetes devices and the need for healthcare professionals to stay updated with technological advancements in diabetes care【7:1†source】.\n",
      "\n",
      "These examples illustrate the ongoing innovation in medical technology, particularly in diabetes management, which is a critical area of focus in recent R&D efforts.\n",
      "\n",
      "👤 **User:** What are the key features from Dexcom G7 CGM System?\n",
      "\n",
      "🤖 **Agent:** The Dexcom G7 Continuous Glucose Monitoring (CGM) System, cleared by the FDA in December 2022, offers several key features that mark significant advancements over its predecessor, the Dexcom G6:\n",
      "\n",
      "1. **Integrated Design**: The Dexcom G7 combines the transmitter and CGM sensor into a single unit, simplifying the application process.\n",
      "2. **Simplified Applicator**: The system uses a one-click applicator, making it easier to apply the sensor.\n",
      "3. **Reduced Size**: The G7 sensor is significantly smaller than the G6 sensor, increasing comfort and discretion for the user.\n",
      "4. **Rapid Warm-Up Time**: The G7 system has a warm-up time of only 30 minutes, compared to 2 hours with the G6, allowing for quicker start-up.\n",
      "5. **Flexible Placement**: For individuals aged 7 years and older, the sensor can be placed on the back of the upper arm. For children aged between 2 and 6 years, it can be placed on the back of the upper arm or on the upper buttocks【13:1†source】.\n",
      "\n",
      "👤 **User:** What documents are available in SharePoint related to R&D?\n",
      "\n",
      "🤖 **Agent:** The documents related to R&D available in SharePoint cover various aspects of advancements and trends in research and development. Some of the key topics include:\n",
      "\n",
      "1. **Improved Diabetes Technology**: \n",
      "   - Reports on user satisfaction due to simpler device management and potential for better adherence with fewer daily injections.\n",
      "\n",
      "2. **Digital Platforms and Decision Support**:\n",
      "   - Development of cloud-based platforms and secure apps that integrate CGM data, insulin dose logs, dietary patterns, and physical activity into a single interface.\n",
      "   - Features like single dashboards for easy interpretation of glucose trends, virtual clinics for remote monitoring, and AI-powered recommendations for insulin therapy adjustments.\n",
      "\n",
      "3. **Future Directions in CGM and Diabetes Technology**:\n",
      "   - Research on non-invasive or minimally invasive sensors for continuous glucose measurement.\n",
      "   - Efforts to extend the lifespan of CGM devices to function for several months without requiring frequent replacements.\n",
      "   - Enhanced interoperability among devices like CGMs, insulin pens/pumps, and software.\n",
      "\n",
      "These documents illustrate the ongoing efforts and future directions in the field of R&D, particularly focusing on diabetes technology and the integration of digital platforms for better disease management【17:1†source】.\n",
      "\n",
      "👤 **User:** How does Product A compare to Product B in terms of MARD percentage across different glucose ranges?\n",
      "\n",
      "🤖 **Agent:** Product A and Product B are compared based on their Mean Absolute Relative Difference (MARD) percentage, which is a critical measure of accuracy for continuous glucose monitoring (CGM) systems across different glucose ranges.\n",
      "\n",
      "### Product A\n",
      "- **Study Design**: Participants included 40 adults with type 1 and insulin-dependent type 2 diabetes.\n",
      "- **Trial Duration**: Participants wore Product A for a full sensor session (10 days) with a 12-hour grace period.\n",
      "- **Data Collection**: Glucose readings were recorded every 5 minutes, paired with periodic reference measurements.\n",
      "- **Key Outcome**: Product A reported a MARD of 8.7% (±0.4%) relative to reference methods【21:3†source】.\n",
      "\n",
      "### Product B\n",
      "- The specific details for Product B's MARD percentage across different glucose ranges were not provided in the document. However, general observations indicate that earlier CGM models, including Product B, often show measurable divergence from capillary blood glucose values and a trend of variability, particularly at low and high glucose levels【21:1†source】【21:4†source】.\n",
      "\n",
      "In summary, Product A demonstrated a MARD of 8.7%, showing high accuracy and meeting internal targets for non-adjunctive use. The exact MARD percentages for Product B across different glucose ranges were not detailed, but it is suggested that such systems may also experience variability in readings compared to traditional methods. More extensive investigations with larger sample sizes and diverse populations are recommended to further validate these findings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.contents import AuthorRole\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from datetime import timedelta\n",
    "from semantic_kernel.agents.open_ai.run_polling_options import RunPollingOptions\n",
    "\n",
    "dataretrievalagent = AzureAIAgent(\n",
    "    client=project_client,\n",
    "    definition=dataretrievalagent_settings_definition,\n",
    "    polling_options=RunPollingOptions(run_polling_interval=timedelta(seconds=1)),\n",
    ")\n",
    "\n",
    "thread = await project_client.agents.create_thread()\n",
    "\n",
    "USER_INPUTS = [\n",
    "    \"What are the latest trends in R&D?\",\n",
    "    \"What are the key features from Dexcom G7 CGM System?\",\n",
    "    \"What documents are available in SharePoint related to R&D?\",\n",
    "    \"How does Product A compare to Product B in terms of MARD percentage across different glucose ranges?\"]\n",
    "try:\n",
    "    for user_input in USER_INPUTS:\n",
    "        # Add the user input as a chat message\n",
    "        await dataretrievalagent.add_chat_message(thread_id=thread.id, message=user_input)\n",
    "        print(f\"👤 **User:** {user_input}\\n\")\n",
    "        \n",
    "        # Invoke the agent for the specified thread and stream the response\n",
    "        async for content in dataretrievalagent.invoke(thread_id=thread.id):\n",
    "            # Only print non-tool messages\n",
    "            if content.role != AuthorRole.TOOL:\n",
    "                print(f\"🤖 **Agent:** {content.content}\\n\")\n",
    "                \n",
    "except HttpResponseError as e:\n",
    "    try:\n",
    "        error_json = json.loads(e.response.content)\n",
    "        logging.error(f\"❌ **Error Message:** {error_json.get('Message')}\")\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(f\"❌ **Non-JSON Error Content:** {e.response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Creating Azure AI Agents: ValidationInsightsAgent**\n",
    "\n",
    "The ValidationInsightsAgent is designed to validate and cross-check the data retrieved by the DataRetrievalAgent. It accesses external references such as Bing and Azure Cognitive Search to verify and supplement the internal data. Additionally, it leverages highly curated knowledge sources (e.g., Azure AI Search) to ensure the accuracy and truthfulness of the information, using a reflection or validation step to highlight missing or conflicting details.\n",
    "\n",
    "Agent Capabilities:\n",
    "\n",
    "+ ✅ External Reference Verification → Queries Bing and Azure Cognitive Search for real-time validation.\n",
    "+ ✅ Reflection & Validation Step → Iteratively reviews and refines the information received from the DataRetrievalAgent.\n",
    "+ ✅ Curated Knowledge Validation → Uses Azure AI Search to confirm the accuracy and reliability of internal data.\n",
    "\n",
    "For a detailed breakdown of how to create a ValidationInsightsAgent and configure its external tools and connections, please refer to:\n",
    "📌 [01-single-agents-with-azure-ai-agents.ipynb](01-single-agents-with-azure-ai-agents.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 00:29:31,800 - micro - MainProcess - INFO     Retrieved Connection ID for TOOL_CONNECTION_NAME_BING: /subscriptions/47f1c914-e299-4953-a99d-3e34644cfe1c/resourceGroups/rg-zhuoqunliai/providers/Microsoft.MachineLearningServices/workspaces/zhuoqunli-1959/connections/agentsbinggrounding (2375541312.py:get_connection_id:22)\n",
      "INFO:micro:Retrieved Connection ID for TOOL_CONNECTION_NAME_BING: /subscriptions/47f1c914-e299-4953-a99d-3e34644cfe1c/resourceGroups/rg-zhuoqunliai/providers/Microsoft.MachineLearningServices/workspaces/zhuoqunli-1959/connections/agentsbinggrounding\n",
      "2025-03-20 00:29:31,802 - micro - MainProcess - INFO     Bing Grounding Tool added successfully. (3079022251.py:<module>:19)\n",
      "INFO:micro:Bing Grounding Tool added successfully.\n",
      "2025-03-20 00:29:31,805 - micro - MainProcess - INFO     Using PDF file path: C:\\Users\\pablosal\\Desktop\\azure-ai-agent-services-demo\\data\\product_data\\ProductATechncialArchitecture.pdf (3079022251.py:<module>:31)\n",
      "INFO:micro:Using PDF file path: C:\\Users\\pablosal\\Desktop\\azure-ai-agent-services-demo\\data\\product_data\\ProductATechncialArchitecture.pdf\n",
      "2025-03-20 00:29:37,935 - micro - MainProcess - INFO     Azure AI Search Tool added successfully. (3079022251.py:<module>:42)\n",
      "INFO:micro:Azure AI Search Tool added successfully.\n",
      "2025-03-20 00:29:37,939 - micro - MainProcess - INFO     Successfully created ToolSet with Bing and File Search tools. (3079022251.py:<module>:45)\n",
      "INFO:micro:Successfully created ToolSet with Bing and File Search tools.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    BingGroundingTool,\n",
    "    AzureAISearchTool,\n",
    "    FileSearchTool,\n",
    "    VectorStore,\n",
    "    OpenAIFile\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Agent settings\n",
    "validationinsightagent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "# Create a ToolSet to manage tools\n",
    "toolset = ToolSet()\n",
    "\n",
    "try:\n",
    "    # Retrieve and add the Bing Grounding Tool\n",
    "    bing_connection = await get_connection_id(project_client, \"TOOL_CONNECTION_NAME_BING\")\n",
    "    toolset.add(BingGroundingTool(connection_id=bing_connection.id))\n",
    "    logger.info(\"Bing Grounding Tool added successfully.\")\n",
    "\n",
    "    # Retrieve and add the Azure AI Search Tool\n",
    "    # search_connection = await get_connection_id(project_client, \"TOOL_CONNECTION_NAME_SEARCH\")\n",
    "    # azure_ai_search_connection = AzureAISearchTool(\n",
    "    #     index_connection_id=search_connection.id,\n",
    "    #     index_name=\"ai-agentic-index\"\n",
    "    # )\n",
    "    # toolset.add(azure_ai_search_connection)\n",
    "    # logger.info(\"Azure AI Search Tool added successfully.\")\n",
    "\n",
    "    pdf_file_path = r\"C:\\Users\\pablosal\\Desktop\\azure-ai-agent-services-demo\\data\\product_data\\ProductATechncialArchitecture.pdf\"\n",
    "    logger.info(f\"Using PDF file path: {pdf_file_path}\")\n",
    "\n",
    "    file: OpenAIFile = await project_client.agents.upload_file_and_poll(file_path=pdf_file_path, purpose=\"assistants\")\n",
    "    vector_store: VectorStore = await project_client.agents.create_vector_store_and_poll(\n",
    "        file_ids=[file.id], name=\"my_vectorstore\"\n",
    "    )\n",
    "\n",
    "    # 2. Create file search tool with uploaded resources\n",
    "    file_search = FileSearchTool(vector_store_ids=[vector_store.id])\n",
    "\n",
    "    toolset.add(file_search)\n",
    "    logger.info(\"Azure AI Search Tool added successfully.\")\n",
    " \n",
    "\n",
    "    logger.info(\"Successfully created ToolSet with Bing and File Search tools.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create ToolSet: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidationInsightsAgent Run ID: asst_6UraIHhjFuopzUXLcwovSleF\n"
     ]
    }
   ],
   "source": [
    "validationinsightsagent_definition = await project_client.agents.create_agent(\n",
    "    model=dataretrievalagent_settings.model_deployment_name,\n",
    "    name=\"ValidationInsightsAgent\",\n",
    "    description=(\n",
    "        \"An AI agent designed to validate and refine R&D insights by cross-checking internal and external data sources. \"\n",
    "        \"It employs a reflection pattern to ensure response accuracy and relevance.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"You are a validation AI assistant specializing in cross-checking and enhancing insights from internal enterprise data and external sources. \"\n",
    "        \"Prioritize using the internal file search index (vector store) for information retrieval whenever available. \"\n",
    "        \"Leverage tools like Bing, Azure AI Search, and SharePoint to validate and supplement the data. \"\n",
    "        \"Incorporate a reflection step to evaluate data quality, resolve inconsistencies, and refine your responses with proper citations. \"\n",
    "        \"Deliver responses that are accurate, actionable, and insightful.\"\n",
    "    ),\n",
    "    toolset=toolset,\n",
    "    tool_resources=file_search.resources,  # Always use file search vector store tool resources\n",
    "    headers={\"x-ms-enable-preview\": \"true\"},\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    metadata={\n",
    "        \"use_case\": \"Cross-Validation and Insight Generation for R&D\",\n",
    "        \"data_source\": \"Internal (Microsoft Fabric, SharePoint) and External (Bing, Azure AI Search)\",\n",
    "        \"response_validation\": \"Include a reflection step to validate data accuracy and provide citations\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the new agent's run ID\n",
    "print(f\"ValidationInsightsAgent Run ID: {validationinsightsagent_definition.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 **User:** What are the characteristics and architecture of Product A?\n",
      "\n",
      "🤖 **Agent:** The document provided is protected by Microsoft Information Protection and cannot be accessed directly. To proceed, please provide access permissions or use a PDF viewer that supports Azure Rights Management.\n",
      "\n",
      "👤 **User:** What are the key features from Dexcom G7 CGM System?\n",
      "\n",
      "🤖 **Agent:** It appears that the document related to Dexcom G7 CGM System is also protected by Microsoft Information Protection and cannot be accessed directly. To proceed, please provide access permissions or use a PDF viewer that supports Azure Rights Management.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_agent = AzureAIAgent(\n",
    "    client=project_client,\n",
    "    definition=validationinsightsagent_definition,\n",
    "    polling_options=RunPollingOptions(run_polling_interval=timedelta(seconds=1)),\n",
    ")\n",
    "\n",
    "# Create a conversation thread for the agent\n",
    "thread = await project_client.agents.create_thread()\n",
    "\n",
    "# Define a list of user inputs designed to trigger both internal search (e.g., product architecture) \n",
    "# and external market trend validation.\n",
    "USER_INPUTS = [\n",
    "    \"What are the characteristics and architecture of Product A?\",\n",
    "    \"What are the key features from Dexcom G7 CGM System?\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    for user_input in USER_INPUTS:\n",
    "        # Add the user input as a chat message\n",
    "        await validation_agent.add_chat_message(thread_id=thread.id, message=user_input)\n",
    "        print(f\"👤 **User:** {user_input}\\n\")\n",
    "        \n",
    "        # Invoke the agent for the specified thread and stream the response\n",
    "        async for content in validation_agent.invoke(thread_id=thread.id):\n",
    "            # Only print non-tool messages\n",
    "            if content.role != AuthorRole.TOOL:\n",
    "                print(f\"🤖 **Agent:** {content.content}\\n\")\n",
    "                \n",
    "except HttpResponseError as e:\n",
    "    try:\n",
    "        error_json = json.loads(e.response.content)\n",
    "        logger.error(f\"❌ **Error Message:** {error_json.get('Message')}\")\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"❌ **Non-JSON Error Content:** {e.response.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-ai-agent-service-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
