{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use Case Overview**\n",
    "\n",
    "Imagine you’re part of an R&D team that needs to merge structured data (e.g., experimental results, market trends) from Microsoft Fabric with unstructured documents (e.g., research reports, engineering notes) in SharePoint, then validate these findings against external references (e.g., Bing) and a high-quality “ground truth” internal knowledge stores (e.g., Azure AI Search).\n",
    "\n",
    "This was a classic Retrieval-Augmented Generation (RAG) scenario—multiple data sources must be queried in real time and cross-checked for consistency. However, by leveraging Azure AI Foundry Agent Services (an agetic enterprise-ready microservices approach) alongside frameworks like Semantic Kernel, we can evolve beyond basic RAG into a mostly autonomous, agentic system. In this design, Agentic RAG and the Reflection Pattern enable each agent to iteratively refine its output until it’s confident in delivering a high-quality, validated answer—paving the way for intelligent automation that continually learns and improves.\n",
    "\n",
    "To summarize, you’re not only bringing data to the AI but also bringing AI to the data, thus maximizing the value of your knowledge stores. By leveraging state-of-the-art retrieval solutions like Azure AI Search, while also tapping sources such as SharePoint (unstructured data) and Fabric (structured data), you can harness your most valuable asset—data—to achieve new levels of insight and automation. \n",
    "\n",
    "**In this demo, we have two Azure AI Agents (extending beyond a single-agent architecture):**\n",
    "\n",
    "+ DataRetrievalAgent: Has access to Microsoft Fabric (for structured data) and SharePoint (for unstructured documents). Its job is to gather relevant internal data: for example, “failure rates of Material X in high-temperature tests,” or “engineering notes on prior tests.”\n",
    "\n",
    "- ValidationInsightsAgent Has access to Bing / Azure Cognitive Search for external references and can run a “reflection” or “validation” step. Its job is to cross-check what was returned by the first agent and highlight missing or conflicting information. ValidationInsightsAgent has access to highly curated knowledge sources (e.g., Azure AI Search) for validating the accuracy or truthfulness of the information it receives from DataRetrievalAgent.\n",
    "\n",
    "\n",
    "**Moving from a single-agent setup to a multi-agent system is now simpler than ever with Semantic Kernel. The general flow looks like this:**\n",
    "\n",
    "1. The user asks a question (e.g., “Retrieve historical failure rates for Material X in extreme temperatures and cross-check if new standards or conflicting data exist.”).\n",
    "2. The DataRetrievalAgent fetches structured data from Fabric (e.g., lab test results, analytics) and unstructured docs from SharePoint (e.g., research memos, engineering notes).\n",
    "3. The ValidationInsightsAgent then queries Bing/Azure Search to verify or supplement the results. Employ a reflection pattern, where it iterates over the combined results, looking for gaps or inconsistencies. If needed, it loops back to the DataRetrievalAgent for clarifications or additional data.\n",
    "\n",
    "Finally, the user receives a validated, summarized answer that merges internal data with external cross-checks. Thanks to the agents’ back-and-forth reflection.\n",
    "\n",
    "### **Why This Matters**\n",
    "\n",
    "+ **Reduced Manual Research**: Instead of manually sifting through multiple data silos and external search engines, the AI Agents automate data gathering and vetting.\n",
    "+ **Higher Confidence**: Validation ensures data accuracy and highlights missing pieces, improving R&D decision-making.\n",
    "+ **Enterprise-Grade Security**: Each agent can enforce On-Behalf-Of (OBO) authentication to protect sensitive data (e.g., only pulling data the user is authorized to see).\n",
    "In the Jupyter Notebook\n",
    "\n",
    "When you run the code in this Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime as pydatetime\n",
    "from typing import Any, List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from datetime import timedelta\n",
    "\n",
    "# Azure AI Projects\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# semantic kernel\n",
    "from semantic_kernel.contents import AuthorRole\n",
    "from semantic_kernel.agents.azure_ai import AzureAIAgent, AzureAIAgentSettings\n",
    "from semantic_kernel.agents.open_ai.run_polling_options import RunPollingOptions\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# configure logging\n",
    "from utils.ml_logging import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    target_directory = os.getenv(\n",
    "        \"TARGET_DIRECTORY\", os.getcwd()\n",
    "    )  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Client and Load Azure AI Foundry**\n",
    "\n",
    "Here, we initialize the Azure AI client using DefaultAzureCredential. This allows us to authenticate and connect to the Azure AI service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AzureAIAgent.create_client(credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Creating Azure AI Agents: DataRetrievalAgent**\n",
    "\n",
    "The DataRetrievalAgent is responsible for internal data retrieval, combining structured data from Microsoft Fabric with unstructured documents from SharePoint. This agent ensures that research teams can efficiently access critical R&D insights, such as historical failure rates, experimental results, and engineering notes—all while maintaining secure and authorized access controls.\n",
    "\n",
    "Agent Capabilities\n",
    "+ ✅ Structured Data Retrieval → Queries Microsoft Fabric for experiment logs, test results, and structured analytics.\n",
    "+ ✅ Unstructured Document Search → Fetches relevant reports, blueprints, and research notes from SharePoint.\n",
    "+ ✅ OBO Authentication → Uses On-Behalf-Of (OBO) authentication to ensure users can only access data they are permitted to view.\n",
    "\n",
    "For a detailed breakdown of how to create a single Azure AI Agent and configure its data (tools) connections, please refer to:\n",
    "📌 [01-single-agents-with-azure-ai-agents.ipynb](01-single-agents-with-azure-ai-agents.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ServiceRequestError\n",
    "from azure.ai.projects.aio import AIProjectClient\n",
    "\n",
    "\n",
    "async def get_connection_id(client: AIProjectClient, env_var: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Retrieves the connection object using a connection name stored in an environment variable.\n",
    "\n",
    "    Args:\n",
    "        client: The Azure AI Project client.\n",
    "        env_var (str): The environment variable holding the connection name.\n",
    "\n",
    "    Returns:\n",
    "        Connection object if found, otherwise raises an error.\n",
    "    \"\"\"\n",
    "    connection_name = os.getenv(env_var)\n",
    "    if not connection_name:\n",
    "        logger.error(f\"Missing environment variable: '{env_var}'\")\n",
    "        raise ValueError(f\"Environment variable '{env_var}' is required.\")\n",
    "\n",
    "    try:\n",
    "        connection = await client.connections.get(connection_name=connection_name)\n",
    "        logger.info(f\"Retrieved Connection ID for {env_var}: {connection.id}\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to retrieve connection for {env_var}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 22:21:05,118 - micro - MainProcess - INFO     Retrieved Connection ID for TOOL_CONNECTION_NAME_FABRIC: /subscriptions/47f1c914-e299-4953-a99d-3e34644cfe1c/resourceGroups/rg-mukeshag-5297_ai/providers/Microsoft.MachineLearningServices/workspaces/zhuoqunli-5026/connections/glucose_data_fabric (2375541312.py:get_connection_id:22)\n",
      "INFO:micro:Retrieved Connection ID for TOOL_CONNECTION_NAME_FABRIC: /subscriptions/47f1c914-e299-4953-a99d-3e34644cfe1c/resourceGroups/rg-mukeshag-5297_ai/providers/Microsoft.MachineLearningServices/workspaces/zhuoqunli-5026/connections/glucose_data_fabric\n",
      "2025-03-24 22:21:05,121 - micro - MainProcess - INFO     Successfully created ToolSet Fabric tools. (2698616309.py:<module>:17)\n",
      "INFO:micro:Successfully created ToolSet Fabric tools.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    SharepointTool,\n",
    "    FabricTool,\n",
    "    ToolSet,\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Agent settings\n",
    "dataretrievalagent_settings = AzureAIAgentSettings.create()\n",
    "toolset = ToolSet()\n",
    "\n",
    "try:\n",
    "    # Retrieve and add Fabric Tool\n",
    "    fabric_connection = await get_connection_id(\n",
    "        project_client, \"TOOL_CONNECTION_NAME_FABRIC\"\n",
    "    )\n",
    "    toolset.add(FabricTool(connection_id=fabric_connection.id))\n",
    "\n",
    "    logger.info(\"Successfully created ToolSet Fabric tools.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create ToolSet: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FabricDataRetrievalAgent Run ID: asst_z7gyKXpYqR7i7N2qIhZoAT6y\n"
     ]
    }
   ],
   "source": [
    "dataretrievalagent_settings_definition = await project_client.agents.create_agent(\n",
    "    model=dataretrievalagent_settings.model_deployment_name,\n",
    "    name=\"FabricDataRetrievalAgent\",\n",
    "    description=(\n",
    "        \"An AI agent specialized in retrieving and integrating structured data from Microsoft Fabric. \"\n",
    "        \"This includes performance metrics, experiment results, and analytics data. \"\n",
    "        \"The agent is designed to assist in research and development by providing accurate, relevant, and actionable data. \"\n",
    "        \"If no relevant data is found, the agent must clearly indicate this and provide suggestions for alternative queries or data sources.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"### Role & Objective\\n\"\n",
    "        \"You are a research-focused AI assistant responsible for retrieving structured data exclusively from Microsoft Fabric. \"\n",
    "        \"Your goal is to provide precise, well-referenced, and relevant responses to support research and development efforts.\\n\\n\"\n",
    "        \"### Data Retrieval & Prioritization\\n\"\n",
    "        \"1. **Structured Data (Microsoft Fabric):** \\n\"\n",
    "        \"   - Retrieve data from Microsoft Fabric when the query involves numerical metrics, performance statistics, or structured analytics.\\n\"\n",
    "        \"   - Data is available to support comparisons of the accuracy and reliability of two glucose monitoring products (Product A and Product B). \"\n",
    "        \"This structured data evaluates performance across different glucose ranges and includes MARD percentages, accuracy within ±20 mg/dL/±20%, and the number of readings for each product.\\n\"\n",
    "        \"   - Example: clinical glucose monitoring studies between Product A or Product B.\\n\\n\"\n",
    "        \"2. **Integrated Queries:** \\n\"\n",
    "        \"   - If the query requires multiple structured metrics, retrieve and integrate the results for a comprehensive response.\\n\"\n",
    "        \"   - Ensure clarity in presenting combined insights.\\n\\n\"\n",
    "        \"3. **Fallback Behavior:** \\n\"\n",
    "        \"   - If no relevant data is found in Microsoft Fabric, respond with:\\n\"\n",
    "        \"     - A clear statement that no relevant data was found.\\n\"\n",
    "        \"     - Suggestions for alternative queries or data sources (if applicable).\\n\"\n",
    "        \"     - Example: 'No relevant data was found for the requested query in Microsoft Fabric. Consider refining your query or exploring other data sources.'\\n\\n\"\n",
    "        \"### Response Quality\\n\"\n",
    "        \"1. **Accuracy & Relevance:** Always prioritize retrieving the most current and applicable data from Microsoft Fabric.\\n\"\n",
    "        \"2. **Clarity & Transparency:** Clearly indicate the data source(s) used and any limitations in the available information.\\n\"\n",
    "        \"3. **Fallback Handling:** If no relevant data is found, provide a professional and helpful fallback response as outlined above.\\n\"\n",
    "        \"4. **Professionalism:** Present findings in a structured and concise manner to facilitate decision-making.\\n\"\n",
    "    ),\n",
    "    toolset=toolset,\n",
    "    headers={\"x-ms-enable-preview\": \"true\"},\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    metadata={\n",
    "        \"use_case\": \"Structured Data Retrieval for R&D\",\n",
    "        \"data_source\": \"Microsoft Fabric (structured metrics only)\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the agent's run ID (agent ID)\n",
    "print(f\"FabricDataRetrievalAgent Run ID: {dataretrievalagent_settings_definition.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def process_citations(content) -> str:\n",
    "    \"\"\"\n",
    "    Return content plus any citations in Markdown format, ensuring no duplicated citations.\n",
    "\n",
    "    Args:\n",
    "        content: The content object retrieved from the agent.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the agent's text plus any unique citations in Markdown format.\n",
    "    \"\"\"\n",
    "    combined_content = content.content\n",
    "\n",
    "    if hasattr(content, \"items\"):\n",
    "        unique_citation_set = set()\n",
    "        for item in content.items:\n",
    "            if item.content_type == \"annotation\" and item.url:\n",
    "                unique_citation_set.add((item.quote, item.url))\n",
    "\n",
    "        if unique_citation_set:\n",
    "            combined_content += \"\\n\\n**Citations:**\\n\"\n",
    "            for quote, url in unique_citation_set:\n",
    "                combined_content += f\"- **Quote**: {quote}  \\n\"\n",
    "                combined_content += f\"  **URL**: [{url}]({url})\\n\"\n",
    "\n",
    "    return combined_content\n",
    "\n",
    "\n",
    "async def run_agent(project_client, agent_id: str, user_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Runs the data-retrieval agent to process a single user input and returns\n",
    "    a single string containing both user input and the agent responses.\n",
    "\n",
    "    Args:\n",
    "        project_client: A client object for interacting with Azure AI project resources.\n",
    "        agent_id: The ID of the agent to retrieve from project_client.\n",
    "        user_input: A string representing the user's query.\n",
    "\n",
    "    Returns:\n",
    "        A single string with user input and each agent response (enriched with citations).\n",
    "    \"\"\"\n",
    "    # Initialize the string to include the user query\n",
    "    responses = f\"👤 User: {user_input}\\n\"\n",
    "\n",
    "    # Fetch the agent definition using the provided ID\n",
    "    agent_definition = await project_client.agents.get_agent(agent_id)\n",
    "\n",
    "    # Create the AzureAIAgent instance\n",
    "    agent = AzureAIAgent(\n",
    "        client=project_client,\n",
    "        definition=agent_definition,\n",
    "        polling_options=RunPollingOptions(run_polling_interval=timedelta(seconds=1)),\n",
    "    )\n",
    "\n",
    "    # Create a new conversation thread\n",
    "    thread = await project_client.agents.create_thread()\n",
    "\n",
    "    try:\n",
    "        # Send user input to the agent\n",
    "        await agent.add_chat_message(thread_id=thread.id, message=user_input)\n",
    "\n",
    "        # Stream non-tool responses from the agent\n",
    "        async for content in agent.invoke(thread_id=thread.id):\n",
    "            enriched_content = process_citations(content)\n",
    "            responses += f\"\\n🤖 {agent.name}: {enriched_content}\"\n",
    "\n",
    "    except HttpResponseError as e:\n",
    "        try:\n",
    "            error_json = json.loads(e.response.content)\n",
    "            message = error_json.get(\"Message\", \"No error message found\")\n",
    "            logging.error(f\"❌ **HTTP Error:** {message}\")\n",
    "            responses += f\"\\n❌ **HTTP Error:** {message}\"\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(f\"❌ **Non-JSON Error Content:** {e.response.content}\")\n",
    "            responses += f\"\\n❌ **Non-JSON Error Content:** {e.response.content}\"\n",
    "\n",
    "    return responses, thread.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"In which glucose ranges does Product A underperform compared to Product B, and what clinical impact could this have?\"\n",
    "fabric_response, threadID = await run_agent(\n",
    "    project_client, \"asst_iIMaKxWz4JUJ5YyWqoSR9t3n\", user_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Azure AI Agents: SharePointDataRetrievalAgent**\n",
    "\n",
    "The DataRetrievalAgent is responsible for internal data retrieval, combining structured data from Microsoft Fabric with unstructured documents from SharePoint. This agent ensures that research teams can efficiently access critical R&D insights, such as historical failure rates, experimental results, and engineering notes—all while maintaining secure and authorized access controls.\n",
    "\n",
    "Agent Capabilities\n",
    "+ ✅ Structured Data Retrieval → Queries Microsoft Fabric for experiment logs, test results, and structured analytics.\n",
    "+ ✅ Unstructured Document Search → Fetches relevant reports, blueprints, and research notes from SharePoint.\n",
    "+ ✅ OBO Authentication → Uses On-Behalf-Of (OBO) authentication to ensure users can only access data they are permitted to view.\n",
    "\n",
    "For a detailed breakdown of how to create a single Azure AI Agent and configure its data (tools) connections, please refer to:\n",
    "📌 [01-single-agents-with-azure-ai-agents.ipynb](01-single-agents-with-azure-ai-agents.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:55:01,855 - micro - MainProcess - INFO     Retrieved Connection ID for TOOL_CONNECTION_NAME_SHAREPOINT: /subscriptions/47f1c914-e299-4953-a99d-3e34644cfe1c/resourceGroups/rg-mukeshag-5297_ai/providers/Microsoft.MachineLearningServices/workspaces/zhuoqunli-5026/connections/ContosoAgentDemoSharepoint (2375541312.py:get_connection_id:22)\n",
      "INFO:micro:Retrieved Connection ID for TOOL_CONNECTION_NAME_SHAREPOINT: /subscriptions/47f1c914-e299-4953-a99d-3e34644cfe1c/resourceGroups/rg-mukeshag-5297_ai/providers/Microsoft.MachineLearningServices/workspaces/zhuoqunli-5026/connections/ContosoAgentDemoSharepoint\n",
      "2025-03-24 19:55:01,862 - micro - MainProcess - INFO     Successfully created ToolSet with SharePoint and Fabric tools. (2844402077.py:<module>:16)\n",
      "INFO:micro:Successfully created ToolSet with SharePoint and Fabric tools.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    SharepointTool,\n",
    "    ToolSet,\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Agent settings\n",
    "dataretrievalagent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "toolset = ToolSet()\n",
    "\n",
    "try:\n",
    "    # Retrieve and add SharePoint Tool\n",
    "    sharepoint_connection = await get_connection_id(\n",
    "        project_client, \"TOOL_CONNECTION_NAME_SHAREPOINT\"\n",
    "    )\n",
    "    toolset.add(SharepointTool(connection_id=sharepoint_connection.id))\n",
    "\n",
    "    logger.info(\"Successfully created ToolSet with SharePoint and Fabric tools.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create ToolSet: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SharePointDataRetrievalAgent Run ID: asst_x73A5QElh86JNelOI50PgH7T\n"
     ]
    }
   ],
   "source": [
    "dataretrievalagent_settings_definition = await project_client.agents.create_agent(\n",
    "    model=dataretrievalagent_settings.model_deployment_name,\n",
    "    name=\"SharePointDataRetrievalAgent\",\n",
    "    description=(\n",
    "        \"An AI agent specialized in retrieving and analyzing unstructured documents from SharePoint. \"\n",
    "        \"This includes research papers, legal documents, and product engineering files (PDFs). \"\n",
    "        \"The agent is designed to assist in research and development by providing accurate, relevant, and actionable insights. \"\n",
    "        \"If no relevant data is found, the agent must clearly indicate this and provide suggestions for alternative queries or data sources.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"### Role & Objective\\n\"\n",
    "        \"You are a research-focused AI assistant responsible for retrieving and analyzing unstructured documents exclusively from SharePoint. \"\n",
    "        \"Your goal is to provide precise, well-referenced, and relevant responses to support research and development, legal analysis, and product engineering efforts.\\n\\n\"\n",
    "        \"### Data Retrieval & Prioritization\\n\"\n",
    "        \"1. **Unstructured Data (SharePoint):** \\n\"\n",
    "        \"   - Retrieve documents from SharePoint when the query involves research papers, legal documents, or product engineering files (PDFs).\\n\"\n",
    "        \"   - Focus on extracting key insights, summaries, and actionable information from the retrieved documents.\\n\"\n",
    "        \"   - Example: 'Retrieve research papers on Material X used in high-temperature environments,' or 'Find legal documents related to patent filings for Product A.'\\n\\n\"\n",
    "        \"2. **Document Types:** \\n\"\n",
    "        \"   - Research Papers: Summarize findings, methodologies, and conclusions.\\n\"\n",
    "        \"   - Legal Documents: Extract key clauses, compliance requirements, and patent-related information.\\n\"\n",
    "        \"   - Product Engineering Files: Highlight design notes, test results, and engineering decisions.\\n\\n\"\n",
    "        \"3. **Integrated Queries:** \\n\"\n",
    "        \"   - If the query spans multiple document types, retrieve and integrate the results for a comprehensive response.\\n\"\n",
    "        \"   - Ensure clarity in presenting combined insights.\\n\\n\"\n",
    "        \"4. **Fallback Behavior:** \\n\"\n",
    "        \"   - If no relevant data is found in SharePoint, respond with:\\n\"\n",
    "        \"     - A clear statement that no relevant data was found.\\n\"\n",
    "        \"     - Suggestions for alternative queries or data sources (if applicable).\\n\"\n",
    "        \"     - Example: 'No relevant data was found for the requested query in SharePoint. Consider refining your query or exploring other data sources.'\\n\\n\"\n",
    "        \"### Response Quality\\n\"\n",
    "        \"1. **Accuracy & Relevance:** Always prioritize retrieving the most current and applicable documents from SharePoint.\\n\"\n",
    "        \"2. **Clarity & Transparency:** Clearly indicate the data source(s) used and any limitations in the available information.\\n\"\n",
    "        \"3. **Fallback Handling:** If no relevant data is found, provide a professional and helpful fallback response as outlined above.\\n\"\n",
    "        \"4. **Professionalism:** Present findings in a structured and concise manner to facilitate decision-making.\\n\"\n",
    "        \"5. **Document Context:** Ensure that extracted insights are presented with sufficient context to maintain their relevance and accuracy.\\n\"\n",
    "    ),\n",
    "    toolset=toolset,\n",
    "    headers={\"x-ms-enable-preview\": \"true\"},\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    metadata={\n",
    "        \"use_case\": \"Unstructured Data Retrieval for R&D, Legal, and Engineering\",\n",
    "        \"data_source\": \"SharePoint\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the agent's run ID (agent ID)\n",
    "print(\n",
    "    f\"SharePointDataRetrievalAgent Run ID: {dataretrievalagent_settings_definition.id}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"In which glucose ranges does Product A underperform compared to Product B, and what clinical impact could this have?\"\n",
    "sharepoint_response, threadID = await run_agent(\n",
    "    project_client, \"asst_x73A5QElh86JNelOI50PgH7T\", user_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 16:46:45,869 - micro - MainProcess - INFO     Retrieved Connection ID for TOOL_CONNECTION_NAME_BING: /subscriptions/47f1c914-e299-4953-a99d-3e34644cfe1c/resourceGroups/rg-mukeshag-5297_ai/providers/Microsoft.MachineLearningServices/workspaces/zhuoqunli-5026/connections/agentsbinggrounding (2375541312.py:get_connection_id:22)\n",
      "INFO:micro:Retrieved Connection ID for TOOL_CONNECTION_NAME_BING: /subscriptions/47f1c914-e299-4953-a99d-3e34644cfe1c/resourceGroups/rg-mukeshag-5297_ai/providers/Microsoft.MachineLearningServices/workspaces/zhuoqunli-5026/connections/agentsbinggrounding\n",
      "2025-03-24 16:46:45,872 - micro - MainProcess - INFO     Bing Grounding Tool added successfully. (1038912367.py:<module>:19)\n",
      "INFO:micro:Bing Grounding Tool added successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    BingGroundingTool,\n",
    "    AzureAISearchTool,\n",
    "    FileSearchTool,\n",
    "    VectorStore,\n",
    "    OpenAIFile,\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Agent settings\n",
    "validationinsightagent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "# Create a ToolSet to manage tools\n",
    "toolset = ToolSet()\n",
    "\n",
    "try:\n",
    "    # Retrieve and add the Bing Grounding Tool\n",
    "    bing_connection = await get_connection_id(\n",
    "        project_client, \"TOOL_CONNECTION_NAME_BING\"\n",
    "    )\n",
    "    toolset.add(BingGroundingTool(connection_id=bing_connection.id))\n",
    "    logger.info(\"Bing Grounding Tool added successfully.\")\n",
    "\n",
    "    # Retrieve and add the Azure AI Search Tool\n",
    "    # search_connection = await get_connection_id(project_client, \"TOOL_CONNECTION_NAME_SEARCH\")\n",
    "    # azure_ai_search_connection = AzureAISearchTool(\n",
    "    #     index_connection_id=search_connection.id,\n",
    "    #     index_name=\"ai-agentic-index\"\n",
    "    # )\n",
    "    # toolset.add(azure_ai_search_connection)\n",
    "    # logger.info(\"Azure AI Search Tool added successfully.\")\n",
    "\n",
    "    # Dynamically construct the PDF file path using os.path.join\n",
    "    # pdf_file_path = os.path.join(target_directory, \"data\", \"product_data\", \"ProductATechncialArchitecture.pdf\")\n",
    "    # logger.info(f\"Using PDF file path: {pdf_file_path}\")\n",
    "\n",
    "    # file: OpenAIFile = await project_client.agents.upload_file_and_poll(file_path=pdf_file_path, purpose=\"assistants\")\n",
    "    # vector_store: VectorStore = await project_client.agents.create_vector_store_and_poll(\n",
    "    #     file_ids=[file.id], name=\"my_vectorstore\"\n",
    "    # )\n",
    "\n",
    "    # # 2. Create file search tool with uploaded resources\n",
    "    # file_search = FileSearchTool(vector_store_ids=[vector_store.id])\n",
    "\n",
    "    # toolset.add(file_search)\n",
    "    # logger.info(\"Azure AI Search Tool added successfully.\")\n",
    "\n",
    "    # logger.info(\"Successfully created ToolSet with Bing and File Search tools.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create ToolSet: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BingDataRetrievalAgent Run ID: asst_1efh8YZc1dz6TvTpdi2qH9tR\n"
     ]
    }
   ],
   "source": [
    "dataretrievalagent_settings_definition = await project_client.agents.create_agent(\n",
    "    model=dataretrievalagent_settings.model_deployment_name,\n",
    "    name=\"BingDataRetrievalAgent\",\n",
    "    description=(\n",
    "        \"An AI agent specialized in parsing user queries against Bing, refining intent, and gathering \"\n",
    "        \"high-quality information from web-based sources. This includes identifying relevant research, \"\n",
    "        \"news articles, reference documents, and other online data to support research and development. \"\n",
    "        \"If no relevant data is found, the agent must clearly indicate this and suggest potential alternative \"\n",
    "        \"queries or data sources.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"### Role & Objective\\n\"\n",
    "        \"You are a research-focused AI assistant that uses Bing to interpret user queries and retrieve \"\n",
    "        \"quality data from the web. Your goal is to provide precise, relevant, and actionable insights.\\n\\n\"\n",
    "        \"### Data Retrieval & Prioritization\\n\"\n",
    "        \"1. **Web-Based Data (Bing):** \\n\"\n",
    "        \"   - Interpret user queries to refine intent before searching using Bing.\\n\"\n",
    "        \"   - Retrieve relevant articles, academic papers, or any publicly available online data.\\n\"\n",
    "        \"   - Example: 'Find the latest research on Product A's performance in clinical trials' \"\n",
    "        \"or 'Fetch breaking news about emerging technology in thermal materials.'\\n\\n\"\n",
    "        \"2. **Data Integration:**\\n\"\n",
    "        \"   - If the query spans multiple topic areas, unify the results for a comprehensive response.\\n\"\n",
    "        \"   - Provide concise summaries and relevant direct evidence where possible.\\n\\n\"\n",
    "        \"3. **Fallback Behavior:**\\n\"\n",
    "        \"   - If no relevant data is found, respond with:\\n\"\n",
    "        \"     - A clear statement that no relevant data was found via Bing.\\n\"\n",
    "        \"     - Suggestions for query refinement or alternative data sources.\\n\\n\"\n",
    "        \"### Response Quality\\n\"\n",
    "        \"1. **Accuracy & Relevance:** Aim to gather highly credible, up-to-date online sources.\\n\"\n",
    "        \"2. **Clarity & Transparency:** Clearly indicate the data source(s) used and any known limitations.\\n\"\n",
    "        \"3. **Fallback Handling:** Provide a professional, helpful fallback response if no data is found.\\n\"\n",
    "        \"4. **Professionalism:** Present findings in a structured, concise manner to aid decision-making.\\n\"\n",
    "        \"5. **Contextual Summaries:** Ensure extracted insights are summarized with enough context to stay relevant.\\n\"\n",
    "    ),\n",
    "    toolset=toolset,\n",
    "    headers={\"x-ms-enable-preview\": \"true\"},\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    metadata={\n",
    "        \"use_case\": \"Web-Based Data Retrieval via Bing for R&D Insights\",\n",
    "        \"data_source\": \"Bing\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the agent's run ID (the newly created agent's ID)\n",
    "print(f\"BingDataRetrievalAgent Run ID: {dataretrievalagent_settings_definition.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"In which glucose ranges does Product A underperform compared to Product B, and what clinical impact could this have?\"\n",
    "bing_response, thread_id = await run_agent(\n",
    "    project_client, \"asst_1efh8YZc1dz6TvTpdi2qH9tR\", user_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 User: In which glucose ranges does Product A underperform compared to Product B, and what clinical impact could this have?\n",
      "\n",
      "🤖 BingDataRetrievalAgent: To provide an accurate answer, I'll need to look up recent data comparing Product A and Product B in terms of glucose range performance. Please hold on while I gather the information.\n",
      "🤖 BingDataRetrievalAgent: I couldn't find specific data on the glucose ranges where Product A underperforms compared to Product B. You might want to check recent clinical trials or detailed product comparison studies for precise information. If you have more specific details about the products (e.g., their names), I could refine the search further.\n"
     ]
    }
   ],
   "source": [
    "print(bing_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aoai.aoai_helper import AzureOpenAIManager\n",
    "from src.chatapp.prompts import (\n",
    "    generate_user_prompt,\n",
    "    SYSTEM_PROMPT_PLANNER,\n",
    "    SYSTEM_PROMPT_VERIFIER,\n",
    "    generate_verifier_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_manager = AzureOpenAIManager(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_PS\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT_PS\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION_PS\"),\n",
    "    chat_model_name=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID_PS\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In which glucose ranges does Product A underperform compared to Product B, and what clinical impact could this have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 22:35:22,819 - micro - MainProcess - INFO     Function generate_chat_response started at 2025-03-24 22:35:22 (aoai_helper.py:generate_chat_response:379)\n",
      "INFO:micro:Function generate_chat_response started at 2025-03-24 22:35:22\n",
      "2025-03-24 22:35:22,828 - micro - MainProcess - INFO     Sending request to Azure OpenAI at 2025-03-24 22:35:22 (aoai_helper.py:generate_chat_response:436)\n",
      "INFO:micro:Sending request to Azure OpenAI at 2025-03-24 22:35:22\n",
      "2025-03-24 22:35:23,937 - micro - MainProcess - INFO     Function generate_chat_response finished at 2025-03-24 22:35:23 (Duration: 1.12 seconds) (aoai_helper.py:generate_chat_response:490)\n",
      "INFO:micro:Function generate_chat_response finished at 2025-03-24 22:35:23 (Duration: 1.12 seconds)\n"
     ]
    }
   ],
   "source": [
    "agents = await az_manager.generate_chat_response(\n",
    "    query=generate_user_prompt(query),\n",
    "    conversation_history=[],\n",
    "    system_message_content=SYSTEM_PROMPT_PLANNER,\n",
    "    response_format=\"json_object\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FabricDataRetrievalAgent', 'SharePointDataRetrievalAgent']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "agents[\"response\"][\"agents_needed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 22:23:48,654 - micro - MainProcess - INFO     Function generate_chat_response started at 2025-03-24 22:23:48 (aoai_helper.py:generate_chat_response:379)\n",
      "INFO:micro:Function generate_chat_response started at 2025-03-24 22:23:48\n",
      "2025-03-24 22:23:48,662 - micro - MainProcess - INFO     Sending request to Azure OpenAI at 2025-03-24 22:23:48 (aoai_helper.py:generate_chat_response:436)\n",
      "INFO:micro:Sending request to Azure OpenAI at 2025-03-24 22:23:48\n",
      "2025-03-24 22:23:50,680 - micro - MainProcess - INFO     Function generate_chat_response finished at 2025-03-24 22:23:50 (Duration: 2.03 seconds) (aoai_helper.py:generate_chat_response:490)\n",
      "INFO:micro:Function generate_chat_response finished at 2025-03-24 22:23:50 (Duration: 2.03 seconds)\n"
     ]
    }
   ],
   "source": [
    "evaluation = await az_manager.generate_chat_response(\n",
    "    query=generate_verifier_prompt(\n",
    "        query,\n",
    "        fabric_data_summary=fabric_response,\n",
    "        sharepoint_data_summary=sharepoint_response,\n",
    "    ),\n",
    "    conversation_history=[],\n",
    "    system_message_content=SYSTEM_PROMPT_VERIFIER,\n",
    "    response_format=\"json_object\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Denied',\n",
       " 'reason': \"The data from Fabric indicates that Product A underperforms compared to Product B in all glucose ranges, whereas SharePoint data suggests underperformance only in glucose ranges below 70 mg/dL and above 180 mg/dL. This inconsistency in the glucose ranges where Product A underperforms creates ambiguity. Additionally, the clinical impact described needs more comprehensive details for each glucose range to fully address the user's query.\",\n",
       " 'response': '',\n",
       " 'rewritten_query': 'What are the specific glucose ranges where Product A underperforms compared to Product B, and what are the detailed clinical impacts for each range?'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['response'][]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    SharepointTool,\n",
    "    ToolSet,\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Agent settings\n",
    "validationinsightsagent_settings = AzureAIAgentSettings.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidationInsightsAgent Run ID: asst_9Fa5VtMRAjyv8PvArrAy1tAx\n"
     ]
    }
   ],
   "source": [
    "validationinsightsagent_settings_definition = await project_client.agents.create_agent(\n",
    "    model=validationinsightsagent_settings.model_deployment_name,\n",
    "    name=\"ValidationInsightsAgent\",\n",
    "    description=(\n",
    "        \"A cross-referencing AI agent that compares data from two sources: the Fabric agent (structured data) and the \"\n",
    "        \"SharePoint agent (unstructured data). If the information is incomplete or contradictory, the agent returns a refined \"\n",
    "        \"query or additional instructions. If validated, it produces a final, comprehensive response marked 'APPROVED:', \"\n",
    "        \"providing in-depth details and citations from each source.\"\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"### Role & Objective\\n\"\n",
    "        \"You are responsible for verifying data from:\\n\"\n",
    "        \"1. Microsoft Fabric (structured data, via the Fabric Agent).\\n\"\n",
    "        \"2. SharePoint (unstructured data, via the SharePoint Agent).\\n\\n\"\n",
    "        \"Your main goal is to:\\n\"\n",
    "        \" - Ensure that the combined data set is correct, consistent, and sufficient to answer the user's query.\\n\"\n",
    "        \" - If the data is valid, produce a final, detailed response (including citations). \"\n",
    "        \" - If the data is insufficient or conflicting, return a refined query or instructions on what additional data is needed.\\n\\n\"\n",
    "        \"### Validation Flow\\n\"\n",
    "        \"1. **Compare & Contrast (Cross-Check)**:\\n\"\n",
    "        \"   - Examine metrics, text descriptions, and any references from Fabric.\\n\"\n",
    "        \"   - Compare these to the corresponding documents, notes, or references from SharePoint.\\n\"\n",
    "        \"   - Identify any gaps, contradictions, or outdated information.\\n\\n\"\n",
    "        \"2. **Outcome Determination**:\\n\"\n",
    "        \"   - **Refined Query / Retry**:\\n\"\n",
    "        \"     - If data is incomplete or contradictory, list the specific points that are unclear or missing.\\n\"\n",
    "        \"     - Provide a suggested refined query or instructions for how the retrieval agents should update their search.\\n\\n\"\n",
    "        \"   - **Approved & Detailed Response**:\\n\"\n",
    "        \"     - If the data is consistent and sufficient to address the user's query, output a consolidated response.\\n\"\n",
    "        \"     - Begin your final answer with the keyword 'APPROVED:'.\\n\"\n",
    "        \"     - Include a **detailed explanation** of how the data answers the query, referencing each relevant data point.\\n\"\n",
    "        \"     - Present **all applicable citations** or URLs from both Fabric and SharePoint, so the user can see the sources.\\n\"\n",
    "        \"     - Summarize key points, numerical insights, or textual findings in a structured layout (lists, bullet points, etc.).\\n\"\n",
    "        \"     - If needed, include disclaimers or clarifications about any assumptions.\\n\\n\"\n",
    "        \"### Citations & Transparency\\n\"\n",
    "        \"1. **Explicit Source Markers**:\\n\"\n",
    "        \"   - Label each piece of data with its origin (e.g., 'Fabric Query 2025-03-24', or 'SharePoint Document: [URL or title]').\\n\"\n",
    "        \"2. **Provide Hyperlinks or Document IDs**:\\n\"\n",
    "        \"   - Where possible, include direct links to the SharePoint document or an identifier for the Fabric dataset.\\n\"\n",
    "        \"3. **Clarity in Narration**:\\n\"\n",
    "        \"   - The final user-facing summary must show how each citation supports the conclusion.\\n\\n\"\n",
    "        \"### Response Quality\\n\"\n",
    "        \"1. **Completeness**:\\n\"\n",
    "        \"   - If you produce 'APPROVED:' content, it must offer a thorough explanation, ensuring the user sees how the data fully \"\n",
    "        \"answers the query.\\n\"\n",
    "        \"2. **Readability & Professional Tone**:\\n\"\n",
    "        \"   - Keep your explanation and citations organized and clear.\\n\"\n",
    "        \"3. **Fallback Handling**:\\n\"\n",
    "        \"   - If no relevant data is found from either source, explicitly state so and provide suggestions for alternative queries.\\n\"\n",
    "        \"4. **Structured Summaries**:\\n\"\n",
    "        \"   - Utilize concise paragraphs, bullet points, or short headings for clarity.\\n\"\n",
    "    ),\n",
    "    toolset=toolset,\n",
    "    headers={\"x-ms-enable-preview\": \"true\"},\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    metadata={\n",
    "        \"use_case\": \"Cross-Referencing & Validation for R&D Insights\",\n",
    "        \"data_source\": \"Fabric & SharePoint\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"ValidationInsightsAgent Run ID: {validationinsightsagent_settings_definition.id}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = '''\n",
    "👤 User: In which glucose ranges does Product A underperform compared to Product B, and what clinical impact could this have?\n",
    "\n",
    "🤖 SharePointDataRetrievalAgent: Product A underperforms compared to Product B in glucose ranges where interstitial fluid readings differ significantly from capillary blood glucose measurements. This discrepancy can lead to potential miscalculations in insulin dosing, increasing the risk of hypoglycemia or hyperglycemia. Clinically, this could result in adverse events such as severe hypoglycemic episodes or long-term complications from poor glycemic control【3:2†source】【3:3†source】.\n",
    "\n",
    "**Citations:**\n",
    "- **Quote**: 【3:2†source】  \n",
    "  **URL**: [https://microsoft.sharepoint.com/teams/ContosoDemoAgents/Shared Documents/R+D Team/Product A & B Comparison.pdf](https://microsoft.sharepoint.com/teams/ContosoDemoAgents/Shared Documents/R+D Team/Product A & B Comparison.pdf)\n",
    "- **Quote**: 【3:3†source】  \n",
    "  **URL**: [https://microsoft.sharepoint.com/teams/ContosoDemoAgents/Shared Documents/Legal Team/Regulatory Requirements.pdf](https://microsoft.sharepoint.com/teams/ContosoDemoAgents/Shared Documents/Legal Team/Regulatory Requirements.pdf)'''\n",
    "+ \n",
    "''' '''\n",
    "agent_response = await run_agent(project_client, dataretrievalagent_settings_definition, user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Creating Multi Agent System with Semantic Kernel Agentic Framework**\n",
    "\n",
    "The following sample demonstrates how to create an OpenAI assistant using either Azure OpenAI or OpenAI, a chat completion agent and have them participate in a group chat to work towards the user's requirement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import streamlit as st\n",
    "import traceback\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import AgentGroupChat\n",
    "from semantic_kernel.agents.azure_ai import AzureAIAgent\n",
    "from semantic_kernel.agents.strategies import (\n",
    "    KernelFunctionSelectionStrategy,\n",
    "    KernelFunctionTerminationStrategy,\n",
    ")\n",
    "from semantic_kernel.functions import KernelFunctionFromPrompt\n",
    "from semantic_kernel.contents import ChatHistoryTruncationReducer\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent name constants\n",
    "SHAREPOINT_AGENT = \"SharePointDataRetrievalAgent\"\n",
    "FABRIC_AGENT = \"FabricDataRetrievalAgent\"\n",
    "VERIFIER_AGENT = \"ValidationInsightsAgent\"\n",
    "\n",
    "\n",
    "def create_kernel():\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    kernel = Kernel()\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            deployment_name=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "            endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        )\n",
    "    )\n",
    "    return kernel\n",
    "\n",
    "\n",
    "selection_function = KernelFunctionFromPrompt(\n",
    "    function_name=\"selection\",\n",
    "    prompt=f\"\"\"\n",
    "    You are the routing orchestrator. Examine the provided conversation and determine which participant\n",
    "    should speak next. Always respond with the name of exactly one participant, and no explanation.\n",
    "\n",
    "    The participants are:\n",
    "    - {SHAREPOINT_AGENT}\n",
    "    - {FABRIC_AGENT}\n",
    "    - {VERIFIER_AGENT}\n",
    "\n",
    "    Conversation History:\n",
    "    {{{{$history}}}}\n",
    "\n",
    "    Routing Rules (simplified example):\n",
    "    1. If the last speaker is the User, choose {SHAREPOINT_AGENT}.\n",
    "    2. If the last speaker is {SHAREPOINT_AGENT}, choose {FABRIC_AGENT}.\n",
    "    3. If the last speaker is {FABRIC_AGENT}, choose {VERIFIER_AGENT}.\n",
    "    4. If the last speaker is {VERIFIER_AGENT}, choose {SHAREPOINT_AGENT}.\n",
    "\n",
    "    Important:\n",
    "    - Do not choose the same participant who just spoke last.\n",
    "    - Return only the name of the next participant as plain text.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "termination_function = KernelFunctionFromPrompt(\n",
    "    function_name=\"termination\",\n",
    "    prompt=f\"\"\"\n",
    "    You are the workflow terminator. Determine if the ValidationInsightsAgent has fully approved\n",
    "    the retrieved information.\n",
    "\n",
    "    Conversation History:\n",
    "    {{$history}}\n",
    "\n",
    "    - If the final message from {VERIFIER_AGENT} contains the keyword \"APPROVED:\", output \"yes\".\n",
    "    - Otherwise, output \"no\".\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# Kernel Creation\n",
    "def create_kernel() -> Kernel:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    kernel = Kernel()\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            deployment_name=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "            endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        )\n",
    "    )\n",
    "    return kernel\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def debug_result_parser(result):\n",
    "    logging.debug(f\"Selection function output: {result}\")\n",
    "    return str(result.value[0]).strip() if result.value[0] is not None else FABRIC_AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Logging\n",
    "def log_agent_invocation(agent_name, role, input_data, output_data, metadata):\n",
    "    trace_data = {\n",
    "        \"agent_name\": agent_name,\n",
    "        \"role\": role,\n",
    "        \"input\": input_data,\n",
    "        \"output\": output_data,\n",
    "        \"metadata\": metadata,\n",
    "    }\n",
    "    logger.info(\"AGENT INVOCATION:\\n\" + json.dumps(trace_data, indent=4))\n",
    "\n",
    "\n",
    "# Main Async Function\n",
    "async def main():\n",
    "    async with DefaultAzureCredential() as creds, AzureAIAgent.create_client(\n",
    "        credential=creds\n",
    "    ) as client:\n",
    "        # Define agent IDs\n",
    "\n",
    "        # Agent name constants\n",
    "        agents_ids = {\n",
    "            SHAREPOINT_AGENT: \"asst_5eeFoBOTSod13UeN6eOIdMrs\",\n",
    "            FABRIC_AGENT: \"asst_gKCbHiEH1iGiEc4YRla1VFZ2\",\n",
    "            VERIFIER_AGENT: \"asst_w9E6NQ12RjMkWAVJ1lvMM3UE\",\n",
    "        }\n",
    "\n",
    "        # Initialize our four agents only once\n",
    "        agents = {}\n",
    "        for name, agent_id in agents_ids.items():\n",
    "            definition = await client.agents.get_agent(agent_id)\n",
    "            agents[name] = AzureAIAgent(client=client, definition=definition)\n",
    "\n",
    "        # Build multi-agent chat in session state if not present\n",
    "        chat = AgentGroupChat(\n",
    "            agents=list(agents.values()),\n",
    "            selection_strategy=KernelFunctionSelectionStrategy(\n",
    "                function=selection_function,\n",
    "                kernel=create_kernel(),\n",
    "                result_parser=debug_result_parser,\n",
    "                agent_variable_name=\"agents\",\n",
    "                history_variable_name=\"history\",\n",
    "            ),\n",
    "            termination_strategy=KernelFunctionTerminationStrategy(\n",
    "                agents=[agents[VERIFIER_AGENT]],\n",
    "                function=termination_function,\n",
    "                kernel=create_kernel(),\n",
    "                result_parser=lambda result: \"yes\" in str(result.value[0]).lower(),\n",
    "                history_variable_name=\"history\",\n",
    "                maximum_iterations=6,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Omni system prompt\n",
    "        SYSTEM_MESSAGE = \"\"\"\n",
    "        You are an intelligent multi-agent R&D assistant designed to help Product Managers quickly access, integrate, and evaluate information from multiple specialized sources.\n",
    "        Aim to support fast and accurate decision-making for R&D insights, leveraging internal and external data comprehensively and effectively.\n",
    "        \"\"\"\n",
    "        await chat.add_chat_message(SYSTEM_MESSAGE)\n",
    "\n",
    "        user_message = \"In which glucose ranges does Product A underperform compared to Product B, and what clinical impact could this have?\"\n",
    "        await chat.add_chat_message(message=user_message)\n",
    "        print(f\"👤 User: {user_message}\\n\")\n",
    "        try:\n",
    "            async for content in chat.invoke():\n",
    "                agent_name = content.name or \"Unknown\"\n",
    "                agent_role = content.role.name\n",
    "                agent_output = content.content\n",
    "                # Extracting citations clearly from AnnotationContent items\n",
    "                citations = []\n",
    "                for item in content.items:\n",
    "                    if item.content_type == \"annotation\":\n",
    "                        citation = {\"quote\": item.quote, \"url\": item.url}\n",
    "                        citations.append(citation)\n",
    "                # structured information clearly\n",
    "                print(f\"AGENT ROLE: {agent_role}\")\n",
    "                print(f\"AGENT NAME: {agent_name}\")\n",
    "                print(\"OUTPUT:\", agent_output)\n",
    "                print(\"CITATIONS:\")\n",
    "                for citation in citations:\n",
    "                    print(f\"  - Quote: {citation['quote']}, URL: {citation['url']}\")\n",
    "                print(\"-\" * 80)\n",
    "\n",
    "        except HttpResponseError as err:\n",
    "            logger.error(f\"Http Error during conversation: {err}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"General Error during conversation: {e}\")\n",
    "\n",
    "        finally:\n",
    "            chat.is_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 User: In which glucose ranges does Product A underperform compared to Product B, and what clinical impact could this have?\n",
      "\n",
      "AGENT ROLE: ASSISTANT\n",
      "AGENT NAME: SharePointDataRetrievalAgent\n",
      "OUTPUT: Product A underperforms compared to Product B in glucose ranges where it consistently records lower glucose levels than the reference glucometer. Specifically, while Product B tends to overestimate, Product A tends to underestimate glucose levels【4:1†source】.\n",
      "\n",
      "The clinical impact of this underperformance is significant. Inaccurate glucose readings can lead to inappropriate insulin dosing decisions, potentially resulting in hypoglycemia if glucose levels are underestimated. This is especially critical in situations requiring precise glucose control【4:1†source】.\n",
      "CITATIONS:\n",
      "  - Quote: 【4:1†source】, URL: https://microsoft.sharepoint.com/teams/ContosoDemoAgents/Shared Documents/R+D Team/Product A & B Comparison.pdf\n",
      "  - Quote: 【4:1†source】, URL: https://microsoft.sharepoint.com/teams/ContosoDemoAgents/Shared Documents/R+D Team/Product A & B Comparison.pdf\n",
      "--------------------------------------------------------------------------------\n",
      "AGENT ROLE: ASSISTANT\n",
      "AGENT NAME: FabricDataRetrievalAgent\n",
      "OUTPUT: Product A underperforms compared to Product B in glucose ranges where it consistently records lower glucose levels than the reference glucometer. Specifically, while Product B tends to overestimate, Product A tends to underestimate glucose levels .\n",
      "\n",
      "The clinical impact of this underperformance is significant. Inaccurate glucose readings can lead to inappropriate insulin dosing decisions, potentially resulting in hypoglycemia if glucose levels are underestimated. This is especially critical in situations requiring precise glucose control .\n",
      "CITATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "AGENT ROLE: ASSISTANT\n",
      "AGENT NAME: ValidationInsightsAgent\n",
      "OUTPUT: To provide a complete answer, I need more details about the specific glucose ranges and clinical impacts from either structured data or documents. Could you provide additional context or specify where I should look for this information?\n",
      "CITATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "AGENT ROLE: ASSISTANT\n",
      "AGENT NAME: SharePointDataRetrievalAgent\n",
      "OUTPUT: Product A underperforms compared to Product B in glucose ranges where it consistently records lower glucose levels than the reference glucometer. Specifically, while Product B tends to overestimate, Product A tends to underestimate glucose levels【4:1†source】.\n",
      "\n",
      "The clinical impact of this underperformance is significant. Inaccurate glucose readings can lead to inappropriate insulin dosing decisions, potentially resulting in hypoglycemia if glucose levels are underestimated. This is especially critical in situations requiring precise glucose control【4:1†source】.\n",
      "CITATIONS:\n",
      "  - Quote: 【4:1†source】, URL: https://microsoft.sharepoint.com/teams/ContosoDemoAgents/Shared Documents/R+D Team/Product A & B Comparison.pdf\n",
      "  - Quote: 【4:1†source】, URL: https://microsoft.sharepoint.com/teams/ContosoDemoAgents/Shared Documents/R+D Team/Product A & B Comparison.pdf\n",
      "--------------------------------------------------------------------------------\n",
      "AGENT ROLE: ASSISTANT\n",
      "AGENT NAME: FabricDataRetrievalAgent\n",
      "OUTPUT: Product A underperforms compared to Product B in glucose ranges where it consistently records lower glucose levels than the reference glucometer. Specifically, while Product B tends to overestimate, Product A tends to underestimate glucose levels .\n",
      "\n",
      "The clinical impact of this underperformance is significant. Inaccurate glucose readings can lead to inappropriate insulin dosing decisions, potentially resulting in hypoglycemia if glucose levels are underestimated. This is especially critical in situations requiring precise glucose control .\n",
      "CITATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "AGENT ROLE: ASSISTANT\n",
      "AGENT NAME: ValidationInsightsAgent\n",
      "OUTPUT: To provide a complete answer, I need more details about the specific glucose ranges and clinical impacts from either structured data or documents. Could you provide additional context or specify where I should look for this information?\n",
      "CITATIONS:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-ai-agent-service-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
